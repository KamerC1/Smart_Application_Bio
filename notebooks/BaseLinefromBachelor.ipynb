{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary code usage\n",
    "\n",
    "Highly suggest to use a venv to code this project\n",
    "\n",
    "To create your venv, install venv if necessary:\n",
    "\n",
    "```bash\n",
    "mkdir .venv \n",
    "python3 -m venv .venv\n",
    "source .venv/bin/activate\n",
    "```\n",
    "\n",
    "To install the requirements:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "\n",
    "Make sure to use .venv python as kernel in jupyter notebook\n",
    "\n",
    "```bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pyActigraphy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa57f6a",
   "metadata": {},
   "source": [
    "### Magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d3d7f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '../raw_data/'\n",
    "\n",
    "df = pd.read_csv(data_folder + '1_AHA_RAW.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitude_D = np.sqrt(np.square(df['x_D']) + np.square(df['y_D']) + np.square(df['z_D'])) #mano dominante\n",
    "magnitude_ND = np.sqrt(np.square(df['x_ND']) + np.square(df['y_ND']) + np.square(df['z_ND'])) #mano non dominante"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c2c04f",
   "metadata": {},
   "source": [
    "### Operations with time-series' magnitude\n",
    "- concatenation\n",
    "- difference (btw domaninat and non dominant)\n",
    "- asymmetrix index (ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecac61b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute asimmetry index and difference given magnitude \n",
    "def elaborate_magnitude(operation_type, magnitude_D, magnitude_ND):\n",
    "\n",
    "    elaborated_magnitude = []\n",
    "    \n",
    "    '''\n",
    "    penso sia inutile usare la concatenazione come indice \n",
    "\n",
    "    if operation_type == 'concat':\n",
    "        elaborated_magnitude = pd.concat([magnitude_D, magnitude_ND], ignore_index=True)\n",
    "    '''\n",
    "\n",
    "    if operation_type == 'difference':\n",
    "        elaborated_magnitude = magnitude_D - magnitude_ND\n",
    "    elif operation_type == 'ai':\n",
    "        elaborated_magnitude = (((magnitude_D - magnitude_ND) / (magnitude_D + magnitude_ND)) * 100).fillna(0)\n",
    "    else: \n",
    "        print('operation type non supportata.')\n",
    "        exit(1)\n",
    "\n",
    "    return elaborated_magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create window_size\n",
    "def create_windows(folder, operation_type, WINDOW_SIZE):\n",
    "    series = []\n",
    "    y_AHA = []\n",
    "    y_MACS =[]\n",
    "    y = []\n",
    "    metadata = pd.read_excel(folder + 'metadata2022_04.xlsx')\n",
    "\n",
    "    for j in range (1, metadata.shape[0]+1):\n",
    "        df = pd.read_csv(folder + 'data/' + str(j) + '_AHA_1sec.csv')\n",
    "\n",
    "        #print('Paziente ' + str(j) + ' -> df.shape[0] = ' + str(df.shape[0]))\n",
    "\n",
    "        # Nel caso in cui non bastasse una duplicazione dell'intera time series questa verrà scartata\n",
    "        if df.shape[0]<WINDOW_SIZE:\n",
    "            df_concat = pd.concat([df, df.iloc[:WINDOW_SIZE-df.shape[0]]], ignore_index = True, axis = 0)\n",
    "            df = df_concat\n",
    "            #print('MODIFICATO Paziente ' + str(j) + ' -> df.shape[0] = ' + str(df.shape[0]))\n",
    "\n",
    "        scart = (df.shape[0] % WINDOW_SIZE)/2\n",
    "        \n",
    "        df_cut = df.iloc[math.ceil(scart):df.shape[0]-math.floor(scart)]\n",
    "\n",
    "\n",
    "        # Calculating magnitude\n",
    "        magnitude_D = np.sqrt(np.square(df_cut['x_D']) + np.square(df_cut['y_D']) + np.square(df_cut['z_D']))\n",
    "        magnitude_ND = np.sqrt(np.square(df_cut['x_ND']) + np.square(df_cut['y_ND']) + np.square(df_cut['z_ND']))\n",
    "        for i in range (0, len(magnitude_D), WINDOW_SIZE):\n",
    "            chunk_D = magnitude_D.iloc[i:i + WINDOW_SIZE]\n",
    "            chunk_ND = magnitude_ND.iloc[i:i + WINDOW_SIZE]\n",
    "            series.append(elaborate_magnitude(operation_type, chunk_D, chunk_ND))\n",
    "            y_AHA.append(metadata['AHA'].iloc[j-1])\n",
    "            y_MACS.append(metadata['MACS'].iloc[j-1])\n",
    "            y.append(metadata['hemi'].iloc[j-1]-1)\n",
    "    \n",
    "    return np.array(series), y_AHA, y_MACS, np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fase di chunking (OPTIONAL)\n",
    "#Need to select a window size\n",
    "\n",
    "    for j in range (0, len(magnitude_D), window_size):\n",
    "\n",
    "        chunk_D = magnitude_D.iloc[j:j + window_size]\n",
    "        chunk_ND = magnitude_ND.iloc[j:j + window_size]\n",
    "\n",
    "        if chunk_D.size == window_size and chunk_ND.size == window_size:\n",
    "            \n",
    "            for es in estimators:\n",
    "                es['series'].append(elaborate_magnitude(es['method'], chunk_D, chunk_ND))\n",
    "\n",
    "            if chunk_D.agg('sum') == 0 and chunk_ND.agg('sum') == 0:\n",
    "                to_discard.append(int(j/window_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681ebe48",
   "metadata": {},
   "source": [
    "### Preprocessing starting from raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2385c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nella tesi di triennale non fanno un cazzo di pre-processing, prendono direttamente x, y, z dalle time-series\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e607e87",
   "metadata": {},
   "source": [
    "### Correlation analysis\n",
    "\n",
    "- correlation btw WEEK sessions and Clinical sessions (1h) --> sliding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30945ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nella tesi di triennale usano semplicemente corrcoef ma così non fai sliding\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
